{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmbHptiQoUtO87alwiTa2v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaoSunny124/AI_201/blob/main/API_Parameters_md.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **OpenAI Chat Completion `API Parameters`**üß†üì°‚ú®\n",
        "---\n",
        "- **`When we use API of any LLM model then we use a few API Parameters.API parameters are like special instructions you give to an API. They tell the API exactly what you want it to do.For Exampleüìú :When we ordering pizza üçï online then we choose different crust types , toppings and sizes to make customize our pizza.These choices are like API parameters.`**\n",
        ":\n",
        "### ***`1. Message`***üó®Ô∏è\n",
        "* **`Purpose` :** `This parameter defines the conversation history means define the work or talk of system and the user.`\n",
        "* **`How it works` :** `We provide a list of dictionaries with role and content keys its called \"JSON\" format used storing and exchanging data and It's designed to be easy for both humans and machines to read and write.`\n",
        "   - `role`: `Can be \"system\", \"user\", or \"assistant\".`\n",
        "\n",
        "   - `content`: `The actual text of the message.`\n",
        "```python\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
        "]\n",
        "```\n",
        "\n",
        "### ***`2. Model`***ü§ñ\n",
        "* **`Purpose` :** `Specifies the AI model to use for generating responses.`\n",
        "* **`How it works` :**  `We select a model from OpenAI's offerings (e.g.,üìú gpt-3.5-turbo). Each model has different capabilities and costs.And every LLM have different models , costs and capabilities.`\n",
        "```python\n",
        "model=\"gpt-3.5-turbo\"\n",
        "```\n",
        "\n",
        "### ***`3. Max Completion Tokens`***üéØ\n",
        "* **`Purpose` :** ` Controls the maximum number of tokens utilized or waste in the generated response.`\n",
        "* **`How it works` :** `We set a limit of responses for save the tokkens.A token is roughly equivalent to 4 characters.`\n",
        "```python\n",
        "max_tokens=100\n",
        "```\n",
        "\n",
        "### ***`4. n`***üåÄ\n",
        "* **`Purpose` :** `Specifies the number of different responses to generate.`\n",
        "* **`How it works` :** `Set the \"n\" value greater than 1 , n > 1 , now u will get multiple alternative responses.`\n",
        "```python\n",
        "n=2\n",
        "```\n",
        "\n",
        "### ***`5. Stream`***üåä\n",
        "* **`Purpose` :** `Enables real-time response generation means this makes the conversation from user feel more natural and immediate.`\n",
        "* **`How it works` :** ` Setting stream to True allows you to receive the response token by token as it's generated.This is useful for applications that require immediate feedback and show the display want to display the response as it's being generatedüñãÔ∏è.`\n",
        "```python\n",
        "stream=True\n",
        "```\n",
        "\n",
        "### ***`6. Temperature`***üå°Ô∏è\n",
        "* **`Purpose` :** `Controls the randomness of the generated text.`\n",
        "* **`How it works` :** `Higher temperatures lead to more creative and diverse responses, while lower temperatures produce more focused and deterministic outputs.`\n",
        "```python\n",
        "temperature=0.7\n",
        "```\n",
        "\n",
        "### ***`7. Top_p`***üé≤‚ú®\n",
        "* **`Purpose` :** `Limits the number of tokens considered at each step.`\n",
        "* **`How it works` :** `A higher \"top_p\" value allows for more diverse responses, while a lower value focuses on the most probable or useful tokens.`\n",
        "```python\n",
        "top_p=0.9\n",
        "```\n",
        "\n",
        "### ***`8. Tools`***üõ†Ô∏è\n",
        "* **`Purpose` :** `Enables the use of external tools within the conversation.`\n",
        "* **`How it works` :** `We define a list of tools with their names and descriptions. The model can then suggest using these tools to complete tasks.`\n",
        "```python\n",
        "tools=[\n",
        "    {\"type\": \"function\", \"function\": {\"name\": \"Search\", \"description\": \"Searches the web for information.\"}}\n",
        "]\n",
        "```\n",
        "\n",
        "## **Image of `API Parameters`**\n",
        "---\n",
        "![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GhiHjhySWeUF9XbWxjmSXA.png)\n",
        "\n",
        "---\n",
        "---\n",
        "### **`Thanks for reading!‚ú®`**\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "rnOXsLvtFGdz"
      }
    }
  ]
}